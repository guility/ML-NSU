{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing dataset from Digits\n",
    "from sklearn.datasets import load_digits\n",
    "# sklearn NearestNeighbors clustering and classification algorithms\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Square root function\n",
    "from math import sqrt\n",
    "# Plot library to display images, actually. Now is not used, but if you would uncomment some...\n",
    "import matplotlib.pyplot as plt\n",
    "# Numpy, copy. Don't remember why imported time, but let it be :-\\\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now loading digits\n",
    "digits = load_digits()\n",
    "# Separately loading digits with labels\n",
    "digits_labels = load_digits(return_X_y=True)\n",
    "digits_labels = digits_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "# You've asked for shape of dataset\n",
    "print(digits.data.shape)\n",
    "# It's 64 in a row for every image, so could expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "print(digits.images.shape)\n",
    "# 8*8 for width and height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "# Data method gives us data in a row, and I doesn't want to care about feeding with 2-dimensional data\n",
    "print(digits.data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thought you want unsupervised stuff from us.\n",
    "# In some case it worked. Look through for proofs.\n",
    "# Applying KNN from Sklearn\n",
    "# Fitting\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting distances and indeces\n",
    "distances, indeces = nbrs.kneighbors(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting sparse graph, then reducing it to numbers of sparse cells\n",
    "sparse_graph = nbrs.kneighbors_graph(digits.data).toarray()\n",
    "sparse_nums = []\n",
    "for entry in sparse_graph:\n",
    "    nearest = []\n",
    "    for (i,pos) in enumerate(entry):\n",
    "        if pos == 1:\n",
    "            nearest.append(i)\n",
    "    sparse_nums.append(nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong matches: 21\n",
      "Percentage of wrong matches: < 1 %\n"
     ]
    }
   ],
   "source": [
    "# Actually, that's how it worked:\n",
    "# I'm checking how many times algorithm marked different class objects as nearest.\n",
    "fails = 0\n",
    "for entry in sparse_nums:\n",
    "    if digits_labels[entry[0]]!=digits_labels[entry[1]]:\n",
    "        fails += 1\n",
    "\n",
    "print('Number of wrong matches:', fails)\n",
    "print('Percentage of wrong matches: <', int(100*fails/len(sparse_nums)),'%')\n",
    "\n",
    "# Actually, the best result of that lab for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437\n"
     ]
    }
   ],
   "source": [
    "# Split data into train classes(0-9):\n",
    "# For this - creating the list\n",
    "classes = []\n",
    "\n",
    "# And the lists in it\n",
    "for i in range(10):\n",
    "    classes.append([])\n",
    "\n",
    "# Just checked if int(float) would work properly\n",
    "print(int(len(digits.images)*0.8))\n",
    "\n",
    "# Then for every image of specific digit - put it in specific class for 80% of images.\n",
    "for i in range(int(len(digits.images)*0.8)):\n",
    "    classes[digits_labels[i]].append(digits.data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 size = 143\n",
      "Class 1 size = 146\n",
      "Class 2 size = 142\n",
      "Class 3 size = 146\n",
      "Class 4 size = 144\n",
      "Class 5 size = 145\n",
      "Class 6 size = 144\n",
      "Class 7 size = 143\n",
      "Class 8 size = 141\n",
      "Class 9 size = 143\n",
      "Deviation: 5\n",
      "Relative deviation: ~ 3 %\n"
     ]
    }
   ],
   "source": [
    "# Checking that classes are built correctly:\n",
    "MAX, MIN = 0, 2000\n",
    "for (i,clas) in enumerate(classes):\n",
    "    print('Class',i,'size =',len(clas))\n",
    "    MAX = max(len(clas),MAX)\n",
    "    MIN = min(len(clas),MIN)\n",
    "# Classes are already(and obviously) quite normalized\n",
    "\n",
    "print('Deviation:', MAX-MIN)\n",
    "print('Relative deviation: ~', int(100*(MAX-MIN)/((MAX+MIN)/2)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we could just fit it in binary style. Don't want to spent time on multiclass assosciations.\n",
    "# Let's define euclidian distance for our case\n",
    "def dist(a,b):\n",
    "    summ = 0.0\n",
    "    for i in range(len(a)):\n",
    "        summ += (a[i]-b[i])**2\n",
    "    return sqrt(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking if distance between classes is bigger than between entries of class\n",
    "# The same time - let's create function for creating distances matrix:\n",
    "def distances(data_recs):\n",
    "    distances = []\n",
    "    for (i,record) in enumerate(data_recs):\n",
    "        distances.append([])\n",
    "        for (j,other) in enumerate(data_recs):\n",
    "            distances[i].append(dist(record,other))\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = list(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Calculating distances. Takes forever to perform. Use next cell, if you don't want die old and unsatisfied.\\n# In next cell I just upload distances from file where I've recorded it for you.\\n# Thought also about json storing, but it needs to change IOPub data rate, which is... crap.\\ndistances = distances(data)\\n\\n# Creating file, would be real comma separated value file.\\nf = open('distances.csv', 'w')\\nfor row in distances:\\n    for entry in row:\\n        f.write(str(entry)+',')\\n    f.write('\\n')\\nf.close()\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # Calculating distances. Takes forever to perform. Use next cell, if you don't want die old and unsatisfied.\n",
    "# In next cell I just upload distances from file where I've recorded it for you.\n",
    "# Thought also about json storing, but it needs to change IOPub data rate, which is... crap.\n",
    "distances = distances(data)\n",
    "\n",
    "# Creating file, would be real comma separated value file.\n",
    "f = open('distances.csv', 'w')\n",
    "for row in distances:\n",
    "    for entry in row:\n",
    "        f.write(str(entry)+',')\n",
    "    f.write('\\n')\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading saved values for distances.\n",
    "f = open('distances.csv', 'r')\n",
    "distances = []\n",
    "for line in f:\n",
    "    distance = line.split(',')\n",
    "    distance.pop(distance.index('\\n'))\n",
    "    for i, elem in enumerate(distance):\n",
    "        distance[i] = float(elem)\n",
    "    distances.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 1 number of fails: 16\n",
      "For k = 2 number of fails: 13\n",
      "For k = 3 number of fails: 12\n",
      "For k = 4 number of fails: 12\n",
      "For k = 5 number of fails: 13\n",
      "For k = 6 number of fails: 16\n",
      "For k = 7 number of fails: 16\n",
      "For k = 8 number of fails: 17\n",
      "For k = 9 number of fails: 17\n"
     ]
    }
   ],
   "source": [
    "# Now let's fit the sklearn kNN-classifier\n",
    "X = digits.data\n",
    "y = digits_labels\n",
    "# Splitted for 1500 train and the rest to test\n",
    "border = 1500\n",
    "# There are even some poetry in comments\n",
    "# Running for different number of neighbors:\n",
    "for k in [1,2,3,4,5,6,7,8,9]:\n",
    "    kNN = KNeighborsClassifier(n_neighbors=k)\n",
    "    kNN.fit(X[:border], y[:border])\n",
    "\n",
    "    # Then predict classes for the rest\n",
    "    sk_kNN_labels = kNN.predict(X[border:])\n",
    "\n",
    "    # Evaluating number of fails:\n",
    "    # Haven't got yet how to apply confusion matrix to not-binary classifier.\n",
    "    # Could be applied in correct_class/rest-like approach, but... it would be huge, so...\n",
    "    fails = 0\n",
    "    for i in range(len(y)-border):\n",
    "        if sk_kNN_labels[i]!=y[border+i]:\n",
    "            fails += 1\n",
    "    print('For k = '+str(k)+' number of fails: '+str(fails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Also, accuracy with optimal k-number is: 96%\n",
      "Which is, actualy, very good.\n",
      "Best accuracy achieved in hand-written digits recognition is 98%, but...\n",
      "That accuracy achieved on quite different dataset.\n"
     ]
    }
   ],
   "source": [
    "# Could see optimal k was 3 or 4. This number could differ depending on data.\n",
    "print('Also, accuracy with optimal k-number is: '+str(round(100 - (100*12/(len(y)-1500))))+'%')\n",
    "print('Which is, actualy, very good.')\n",
    "print('Best accuracy achieved in hand-written digits recognition is 98%, but...')\n",
    "print('That accuracy achieved on quite different dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the function, distances included as calculated to fasten process\n",
    "\n",
    "def k_neighb(k, distances):\n",
    "    k_neighbors = []\n",
    "    distance = distances.copy()\n",
    "    distance[distance.index(min(distance))]=np.inf\n",
    "    for i in range(k):\n",
    "        k_neighbors.append(distance.index(min(distance)))\n",
    "        distance[distance.index(min(distance))]=np.inf\n",
    "    return k_neighbors\n",
    "\n",
    "def predict_label(neighbors, label):\n",
    "    cnt = []\n",
    "    for i in range(10):\n",
    "        cnt.append(0)\n",
    "    for neighb in neighbors:\n",
    "        cnt[label[neighb]]+=1\n",
    "    return(cnt.index(max(cnt)))\n",
    "\n",
    "def classifyKNN(X, y, k, distances, border):\n",
    "    pred = []\n",
    "    for entry_num in range(len(y)-border):\n",
    "        neighb = k_neighb(k, distances[entry_num+border])\n",
    "        pred.append(predict_label(neighb, y))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 1 number of fails: 0.020202020202020204\n",
      "For k = 2 number of fails: 0.020202020202020204\n",
      "For k = 3 number of fails: 0.020202020202020204\n",
      "For k = 4 number of fails: 0.020202020202020204\n",
      "For k = 5 number of fails: 0.016835016835016835\n",
      "For k = 6 number of fails: 0.020202020202020204\n",
      "For k = 7 number of fails: 0.030303030303030304\n",
      "For k = 8 number of fails: 0.030303030303030304\n",
      "For k = 9 number of fails: 0.03367003367003367\n"
     ]
    }
   ],
   "source": [
    "# Running my predictor:\n",
    "border = 1500\n",
    "\n",
    "for k in [1,2,3,4,5,6,7,8,9]:\n",
    "    predicted_labels = classifyKNN(X, y, k, distances, border)\n",
    "\n",
    "    fails = 0\n",
    "    for i in range(len(y)-border):\n",
    "        if predicted_labels[i]!=y[border+i]:\n",
    "            fails += 1\n",
    "    print('For k = '+str(k)+' number of fails: '+str(fails/(len(digits_labels)-1500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# My predictor works better. Possibly cause I use floats for calculations, which takes a while, but provides me better accuracy.\n",
    "# Cause sklearn kNN predictor was in \"auto\" mode which must provide best accuracy among different approaches to KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obviously, k number influes my algorithm the same way. Best k couldn't be easily predicted, cause depends on\n",
    "# Number and location of cross-class neighbors, that are closer to some element of other class, than it's real \"classmates\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion table for sklearn algorithm(class0/notclass0):\n",
      "TP = 27, TN = 269, FP = 1, FN = 0\n",
      "Accuracy = 0.9966329966329966\n",
      "Prevalence = 0.09090909090909091\n",
      "Precision = 0.9642857142857143\n",
      "FDR = 0.03571428571428571\n",
      "FOR = 0.0\n",
      "NPV = 1.0\n",
      "TPR, Recall = 1.0\n",
      "FPR = 0.003703703703703704\n",
      "LR+ = 270.0\n",
      "FNR = 0.0\n",
      "TNR = 0.9962962962962963\n",
      "LR- = 0.0\n",
      "DOR - doesn't applicable as far as LR- is equal to zero\n",
      "F1 score = 0.9818181818181817\n"
     ]
    }
   ],
   "source": [
    "# Confusion table in consideration of: class \"0\"/not class \"0\"\n",
    "total = len(y)-border\n",
    "\n",
    "# Sklearn:\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for i,label in enumerate(sk_kNN_labels):\n",
    "    if (label == 0) and (y[border+i]==0):\n",
    "        tp += 1\n",
    "    if (label != 0) and (y[border+i]==0):\n",
    "        fn += 1\n",
    "    if (label == 0) and (y[border+i]!=0):\n",
    "        fp += 1\n",
    "    if (label != 0) and (y[border+i]!=0):\n",
    "        tn += 1\n",
    "\n",
    "print('Confusion table for sklearn algorithm(class0/notclass0):')\n",
    "print('TP = '+ str(tp)+', TN = '+str(tn)+', FP = '+str(fp)+', FN = '+str(fn))\n",
    "print('Accuracy = '+str((tp+tn)/float(total)))\n",
    "print('Prevalence = '+str((tp+fn)/float(total)))\n",
    "precision = (tp)/float(tp+fp)\n",
    "print('Precision = '+str(precision))\n",
    "print('FDR = '+str((fp)/float(tp+fp)))\n",
    "print('FOR = '+str((fn)/float(fn+tn)))\n",
    "print('NPV = '+str((tn)/float(fn+tn)))\n",
    "\n",
    "TPR = (tp)/float(tp+fn)\n",
    "print('TPR, Recall = '+str(TPR))\n",
    "FPR = (fp)/float(fp+tn)\n",
    "print('FPR = '+str(FPR))\n",
    "LRP = TPR/float(FPR)\n",
    "print('LR+ = '+str(LRP))\n",
    "\n",
    "FNR = (fn)/float(tp+fn)\n",
    "TNR = (tn)/float(fp+tn)\n",
    "LRN = FNR/float(TNR)\n",
    "print('FNR = '+str(FNR))\n",
    "print('TNR = '+str(TNR))\n",
    "print('LR- = '+str(LRN))\n",
    "\n",
    "# print('DOR = '+str(LRP/LRN))\n",
    "print(\"DOR - doesn't applicable as far as LR- is equal to zero\")\n",
    "print(\"F1 score = \"+str(2/(1/TPR+1/precision)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion table for my KNN algorithm(class0/notclass0):\n",
      "TP = 27, TN = 270, FP = 0, FN = 0\n",
      "Accuracy = 1.0\n",
      "Prevalence = 0.09090909090909091\n",
      "Precision = 1.0\n",
      "FDR = 0.0\n",
      "FOR = 0.0\n",
      "NPV = 1.0\n",
      "TPR, Recall = 1.0\n",
      "FPR = 0.0\n",
      "LR+ - not applicable, cause FPR = 0\n",
      "FNR = 0.0\n",
      "TNR = 1.0\n",
      "LR- = 0.0\n",
      "DOR - doesn't applicable as far as LR- is equal to zero\n",
      "F1 score = 1.0\n"
     ]
    }
   ],
   "source": [
    "# MY:\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for i,label in enumerate(predicted_labels):\n",
    "    if (label == 0) and (y[border+i]==0):\n",
    "        tp += 1\n",
    "    if (label != 0) and (y[border+i]==0):\n",
    "        fn += 1\n",
    "    if (label == 0) and (y[border+i]!=0):\n",
    "        fp += 1\n",
    "    if (label != 0) and (y[border+i]!=0):\n",
    "        tn += 1\n",
    "\n",
    "print('Confusion table for my KNN algorithm(class0/notclass0):')\n",
    "print('TP = '+ str(tp)+', TN = '+str(tn)+', FP = '+str(fp)+', FN = '+str(fn))\n",
    "print('Accuracy = '+str((tp+tn)/float(total)))\n",
    "print('Prevalence = '+str((tp+fn)/float(total)))\n",
    "precision = (tp)/float(tp+fp)\n",
    "print('Precision = '+str(precision))\n",
    "print('FDR = '+str((fp)/float(tp+fp)))\n",
    "print('FOR = '+str((fn)/float(fn+tn)))\n",
    "print('NPV = '+str((tn)/float(fn+tn)))\n",
    "\n",
    "TPR = (tp)/float(tp+fn)\n",
    "print('TPR, Recall = '+str(TPR))\n",
    "FPR = (fp)/float(fp+tn)\n",
    "print('FPR = '+str(FPR))\n",
    "#LRP = TPR/float(FPR)\n",
    "print('LR+ - not applicable, cause FPR = 0')\n",
    "\n",
    "FNR = (fn)/float(tp+fn)\n",
    "TNR = (tn)/float(fp+tn)\n",
    "LRN = FNR/float(TNR)\n",
    "print('FNR = '+str(FNR))\n",
    "print('TNR = '+str(TNR))\n",
    "print('LR- = '+str(LRN))\n",
    "\n",
    "# print('DOR = '+str(LRP/LRN))\n",
    "print(\"DOR - doesn't applicable as far as LR- is equal to zero\")\n",
    "print(\"F1 score = \"+str(2/(1/TPR+1/precision)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
